---
title: "Youtube Video"
author: Chuyue Xie
output: 
    html_notebook:
    fig_height: 6
    fig_width: 12
---
# Motivation
YouTube has become one of the most popular worldwide video sharing platform nowadays. It includes videos varying from music videos to makeup tutorials, from lifestyle vlog to education videos... YouTube seem be be a hub of creativity and informations. The main purpose of this project is to explore the following questions:
1. What is the most popular comment for the top trending videos in the United States?
2. Is there any difference in people's favor in terms of video types among the United States and korea?

# Data Source
For this project, the dataset used is found at Kaggle published by Mitchell J, a software developer at Backbeat Technologies. The version this project based on is version 115. The following is a weblink to the Kaggle page of the dataset: [Trending YouTube Video Statistics](https://www.kaggle.com/datasnaek/youtube-new). The main dataset used in this project are *USvideos.csv*. Other supporting datasets are *KRvideos.csv*, *US_category_id.json*, *KR_category_id.json*, and 

# Set Up

### Clean up environment and load packages
```{r message=FALSE, paged.print=FALSE}
# clean up the RStudio environment 
rm(list = ls())

# load all packages here: `mosaic`, `tidyverse`, `lubridate`, and all others used
library(mosaic)
library(tidyverse)
library(lubridate)
library(DataComputing)
library(ggplot2)
library(rjson)
```

### Data Intake
The codes below read in data tables from csv files.
```{r message=FALSE, warning=FALSE}
### Load US videos data from USvideos.csv
US_data <- read_csv("USvideos.csv") #full data with 40949 rows
US_sub_data <- # small data with 10k rows
  US_data %>%
  head(10000)
### Load Korean videos data from KRvideos.csv
KR_data <- read_csv("KRvideos.csv") #full data with 34567 rows
KR_sub_data <- # small data with 10k rows
  KR_data %>%
  head(10000)
### Read in the table that includes meaning of each category
US_category_table <- fromJSON(file = "US_category_id.json") #table of category value for US
KR_category_table <- fromJSON(file = "KR_category_id.json") #table of category value for Korea
```

### Data Table Inspection
To start with, the main table with video information for US and korea is shown below:
```{r include=FALSE, paged.print=FALSE}
### US videos dataset
US_data %>%
  glimpse() #the glimpse function will present the columns down the page and rows across

### Korean videos dataset
KR_data %>%
  glimpse()

### US category datatable
print("US category datatable")
US_category_table %>%
  glimpse()

### Korea category datatable
print("Korea category datatable")
KR_category_table %>%
  glimpse()
```
According to the output, there are 16 variables in total for both video datasets. This includes video id, trending date (chr), title (chr), channel title (chr), category id (dbl), publish time (dttm), tags (chr), views (dbl), likes (dbl), dislikes (dbl), comment count (dbl), thumbnail link (chr), comments disabled (lgl), ratings disabled (lgl), and video error or removed (lgl), and description (chr). 

Moreover, for the category datasets, it they look a little messy. There are a 32 levels for US category datasets and 31 levels for the Korea category dataset. Further cleaning of the data table will be conducted in later steps.

In order to decrease the dimension of the tables and prepare it for further analysis, the next step is to transform variable type and clean the data table.

### Data Preprocessing

In this project, the main focus will be place on variables including video id, trending date, channel title, category id, publish time, views, liks, dislikes, and description for US data. For the korea videos data, only video id, trending date, category id, publish time, and views will be used in this project. Other variables will be revoved.

```{r}
### Select variables for the US dataset
US_data <-
  US_data %>%
  select(video_id, trending_date, title, channel_title, category_id, category_id, publish_time, views, likes, dislikes) 
### Select variables for the korea dataset
KR_data <-
  KR_data %>%
  select(video_id, trending_date, category_id, publish_time, views)
```

Notice that trending date and category id are not in the right form. Trending date should be a time variable, but is recognized as a character string. Moreover, category id should be a character string, but is recognized as a number. In the next step, variables trending date and category id are converted into the correct format.

```{r}
### Convert variables for the US video dataset
US_data <-
  US_data %>%
  mutate(trending_date = ydm(trending_date), category_id = as.character(category_id))
### Convert variables for the korea video dataset
KR_data <-
  KR_data %>%
  mutate(trending_date = ydm(trending_date), category_id = as.character(category_id))
```

For the original dataset, the category id variable is named as numbers. In order to give it more meanings, it is transformed into real video category names for furhter analysis.

```{r}
#Step 1: Use the US category table fo create a table for category id&name conversion
id <- rep(c(0), 32) #ccreate a string with length 32 with values equal to 0
name <- rep(c(0), 32) #create a string with length 32 with values equal to 0

### Use US category table to define id and their corresponding movie category name
for (i in 1:32){
  id[i] = US_category_table$items[[i]]$id
  name[i] = US_category_table$items[[i]]$snippet$title
}

### Create a new table that contain the name of each category id
category_name_US <- 
  list(category_id = id, category_name = name) %>%
  as.data.frame()


#Step 2: Use the KR category table fo create a table for category id&name conversion
id_KR <- rep(c(0), 31) #ccreate a string with length 32 with values equal to 0
name_KR <- rep(c(0), 31) #create a string with length 32 with values equal to 0
### Use US category table to define id and their corresponding movie category name
for (i in 1:31){
  id_KR[i] = KR_category_table$items[[i]]$id
  name_KR[i] = KR_category_table$items[[i]]$snippet$title
}

### Create a new table that contain the name of each category id
category_name_KR <- 
  list(category_id = id, category_name = name) %>%
  as.data.frame()

# Step 3: Join the US and KR video tables with the category name tables.
### US videos data
US_data <-
  US_data %>%
  left_join(category_name_US)

### Korea videos data
KR_data <-
  KR_data %>%
  left_join(category_name_KR)
```

The following will show the first few rows of the data to demonstrate the data used in further sections.
```{r}
### US videos data
US_data %>%
  head()

### Korea videos data
KR_data %>%
  head()
```

# Section: Most popular trending video category across time
In this section, the most popular video category among time in the United States and Korea is compared. 

### Step 1: Find the 3 most popular categories for US and Korea
```{r}
### 5 Most Popular Categories for US
US_data %>%
  group_by(category_name) %>%
  summarise(count = n())%>%
  arrange(desc(count)) %>%
  head(3)

### 5 Most Popular Categories for Korea
KR_data %>%
  group_by(category_name) %>%
  summarise(count = n())%>%
  arrange(desc(count)) %>%
  head(3)
```
According to the two tables, among all trending videos, Entertainment videos is the most common for both countries. Moreover, Music and Howto & Style videos appears to be more common in the US while videos about news and politics as well as People & Blogs appear to be more common among trending videos on YouTube in Korea.

### Step 2: Exploratory Data Analysis
Visualization will help to understand this result better. 
```{r}
### list of most popular videos
popular_cate_US <- c("Entertainment", "Music", "Howto & Style")
popular_cate_KR <- c("Entertainment", "People & Blogs", "News & Politics")

### Define Top categories table of the US
Top_5_category_US_count <-
  US_data %>%
  group_by(trending_date, category_name) %>%
  mutate(count = n()) %>%
  filter(category_name %in% popular_cate_US) %>%
  mutate(country = "US")

### Define Top categories table of the Korea
Top_5_category_KR_count <-
  KR_data %>%  
  group_by(trending_date, category_name) %>%
  mutate(count = n()) %>%
  filter(category_name %in% popular_cate_KR) %>%
  mutate(country = "KR")

### Join the two tables and transform it into wide table
Top_5_category_count <-
  rbind(Top_5_category_US_count, Top_5_category_KR_count)

### Plot the graph
Top_5_category_count %>%
  ggplot(aes(x = trending_date, y = count, group = category_name)) +
  geom_line(size = 0.8, alpha = 0.7, aes(color = category_name))+
  facet_grid(country ~.)
```

This plot demonstrates the change in count for top 3 categories of each country from Nov 2018 to June 2019. However, since the daily total video counts are different, the distribution among time may not be shown accurately in this plot. In order to improve this, a daily percent plot for each category is plotted below.

```{r}
### Define Top categories table of the US
Top_5_category_US <-
  US_data %>%
  group_by(trending_date) %>%
  mutate(day_count = n()) %>%
  group_by(trending_date, category_name) %>%
  mutate(cate_count = n(), percent = cate_count / day_count) %>%
  filter(category_name %in% popular_cate_US) %>%
  mutate(country = "US")

### Define Top categories table of the Korea
Top_5_category_KR <-
  KR_data %>%  
  group_by(trending_date) %>%
  mutate(day_count = n()) %>%
  group_by(trending_date, category_name) %>%
  mutate(cate_count = n(), percent = cate_count / day_count) %>%
  filter(category_name %in% popular_cate_KR) %>%
  mutate(country = "KR")

### Join the two tables and transform it into wide table
Top_5_category <-
  rbind(Top_5_category_US, Top_5_category_KR)

### Plot the graph
Top_5_category %>%
  ggplot(aes(x = trending_date, y = percent, group = category_name)) +
  geom_line(size = 0.8, alpha = 0.7, aes(color = category_name))+
  facet_grid(country ~.)

```

According to the plot, the differences in most common categories of trending videos between US and Korea seem appearant. Similar to our observation above, the plot shows that overall, the most common video categories in Korea is Entertainment, News & Politics, and Peopl & Blogs, while in US is Entertainment, Music, and Howto & Styles. 

However, it also shows the change among time. In the subplot for Korea, the percent of count for Entertainment, News & Politics, and Peopl & Blogs started with similar values. However, by the middle of June, there is an increase trend in Entertainment videos percent count. Also, there is a decreasing pattern in People & Blog videos percent count since the beginning of June. 

For the US youtube videos, the Entertainment category appears to relatively stable and take around 25% of all videos. However, the music category boosted in November, December, May, and June, which make match the time of winter and summer breaks.


# Section: Monthly Trending "Title Phrase"

YouTubers always want to get most viewers' attention by making effort in the video titles. In the following analysis, the main purpose is to understand which are the most commonly used phrases among trending YouTube videos. More importantly, how does the use of vocabulary change over time. This analysis will be based on the US dataset.

In order to analysis the montly trend, the dataset will need to be splitted into monthly pieces. Moreover, the title string should be splitted into individual words, which adds up to counts for each vocabulary. The following steps demonstrate this process.

### Step 1: Word Count Function

In order to count the word-use frequency, the following function is constructed below to help with separating the title into vocabularies, removing punctuations, and count the amount of word altogether.

```{r}
word_count <- function(variable){
  variable <- gsub('[[:punct:] ]+',' ',variable) ### remove punctuation
  list <- c() ### empty vector
  num <- c() ### empty vector
  for (i in 1:length(variable)){
    string <- variable[i]
    #print(string)
    words <- tolower(unlist(strsplit(string, " "))) ###split the title string
    #print(words)
    for (j in 1:length(words)){
      #print(j)
      position <- match(words[j], list) ###define the position of the vocab in list
      list_length <- length(list) ###calculate the length of list
      num_len <- length(num) ### calculate the length of number
      if (is.na(position) == FALSE){ ### if item in the list
        num[position] <- num[position]+1 ### add a count to the vocab
      }
      else{ ### ekse if vocab not in the list
        list[list_length+1] = words[j] ###create a new row in list
        num[num_len+1] = 1 ### setnumber of elements to 1 for that vocab
      }
    }
  }
  df <- data.frame(list = list, num = num) ### create a dataframe that combine the list of vocab and their count
  return(df)
}
```

### Step 2: Data Preprocessing
In this step, the entire US dataset is splitted into 8 pieces from November to June
```{r}
Nov <- US_data %>%
  mutate(trending_date = month(trending_date)) %>%
  filter(trending_date == 11)
Dec <- US_data %>%
  mutate(trending_date = month(trending_date)) %>%
  filter(trending_date == 12)
Jan <- US_data %>%
  mutate(trending_date = month(trending_date)) %>%
  filter(trending_date == 1)
Feb <- US_data %>%
  mutate(trending_date = month(trending_date)) %>%
  filter(trending_date == 2)
Mar <- US_data %>%
  mutate(trending_date = month(trending_date)) %>%
  filter(trending_date == 3)
April <- US_data %>%
  mutate(trending_date = month(trending_date)) %>%
  filter(trending_date == 4)
May <- US_data %>%
  mutate(trending_date = month(trending_date)) %>%
  filter(trending_date == 5)
June <- US_data %>%
  mutate(trending_date = month(trending_date)) %>%
  filter(trending_date == 6)
```

### Step 3: Word Count for Each Month
Then, the vocab function is applied to each month's data. The result will be the trending vocab in video titles.
```{r}
Nov_title <- as.vector(Nov$title)
Nov_vocab <- word_count(Nov_title) %>%
  arrange(desc(num)) %>%
  mutate(month = "Nov")

Dec_title <- as.vector(Dec$title)
Dec_vocab <- word_count(Dec_title) %>%
  arrange(desc(num)) %>%
  mutate(month = "Dec")

Jan_title <- as.vector(Jan$title)
Jan_vocab <- word_count(Jan_title) %>%
  arrange(desc(num))%>%
  mutate(month = "Jan")

Feb_title <- as.vector(Feb$title)
Feb_vocab <- word_count(Feb_title) %>%
  arrange(desc(num))%>%
  mutate(month = "Feb")

Mar_title <- as.vector(Mar$title)
Mar_vocab <- word_count(Mar_title) %>%
  arrange(desc(num))%>%
  mutate(month = "Mar")

April_title <- as.vector(April$title)
April_vocab <- word_count(April_title) %>%
  arrange(desc(num))%>%
  mutate(month = "April")

May_title <- as.vector(May$title)
May_vocab <- word_count(May_title) %>%
  arrange(desc(num))%>%
  mutate(month = "May")

June_title <- as.vector(June$title)
June_vocab <- word_count(June_title) %>%
  arrange(desc(num))%>%
  mutate(month = "June")
```
These tables are binded together for further analysis.
```{r}
Vocab_Count <-
  rbind(Nov_vocab, Dec_vocab, Jan_vocab, Feb_vocab, Mar_vocab, April_vocab, May_vocab, June_vocab)
Vocab_Count
```

### Step 4: Data Visualization

Next, a plot showing how top 5 popular vocabularies of each month change from November 2018 to June 2019 is plotted.

```{r}
### List of popular preposition and pronouns that have no actual meaning
prep_n_pron <- "^(the|a|s|at|to|of|with|in|on|and|i|you|for|is|am|are|my|it|ft|from|your|this|the|up|by|his|off)$"

### remove these from the list of popular words
Vocab_Count %>%
  filter(!grepl(prep_n_pron, list)) %>%
  group_by(month) %>%
  top_n(5, num) %>%
  ggplot(aes(x = month, y = num))+
  #geom_point(aes(color = list), size = 2, alpha = 0.5)+
  geom_text(aes(label=list, color = list), angle = 45, alpha = 0.8, nudge_y =1) +
  xlim(c("Nov", "Dec", "Jan", "Feb", "Mar", "April", "May", "June"))+
  ylab("count")

```

According to the plot, the most popular words used in the videos overall is "official". Words such as 2017 and 2018 are used more often by the end of and at the beggining of each year. Moreover, in December, wehn Chirstmas is about to come, the word "Christmas" also become very common in video titles. 

```{r}
popular_cate_US <- c("Entertainment")
### List of most popular words
pattern <- "(official|2017|2018|chistmas|how|music|new|super|trailer|video|vs|bowl)"
US_data %>%
  filter(grepl(pattern = pattern, ignore.case = TRUE, title), category_name %in% popular_cate_US) %>%
  mutate(case = "With Pattern") %>%
  rbind(
    US_data %>%
      filter(!grepl(pattern = pattern, ignore.case = TRUE, title), category_name %in% popular_cate_US) %>%
      mutate(case = "Without Pattern")
  ) %>%
  group_by(trending_date, category_name, case) %>%
  summarise(mean_views = mean(views), se = sd(views)/sqrt(n())) %>%
  #mutate(top = mean_views + 2*se, bottom = mean_views - 2 * se) %>%
  ggplot(aes(x = trending_date, y = mean_views, color = case)) +
  geom_point(alpha = 0.6)+
  geom_line(alpha = 0.6)
```

This plot shows that in general, videos with at least one of the vocabularies (official, 2017, 2018, chistmas, how, music, new, super, trailer, video, vs, bowl) in the tile has higher mean views than videos without the vocaularies. This is especially obvious in December, and after middle March. In order to state with more confidence whether this difference is significant, error bars will be plotted for each day.

```{r}
popular_cate_US <- c("Entertainment")
### List of most popular words
pattern <- "(official|2017|2018|chistmas|how|music|new|super|trailer|video|vs|bowl)"
US_data %>%
  filter(grepl(pattern = pattern, ignore.case = TRUE, title), category_name %in% popular_cate_US) %>%
  mutate(case = "With Pattern") %>%
  rbind(
    US_data %>%
      filter(!grepl(pattern = pattern, ignore.case = TRUE, title), category_name %in% popular_cate_US) %>%
      mutate(case = "Without Pattern")
  ) %>%
  group_by(trending_date, category_name, case) %>%
  summarise(mean_views = mean(views), se = sd(views)/sqrt(n())) %>%
  mutate(top = mean_views + 1.5* se, bottom = mean_views - 1.5 * se) %>%
  ggplot(aes(x = trending_date, y = mean_views, color = case)) +
  geom_point(size = 0.5) + 
  geom_line(size = 0.5)+
  geom_errorbar(aes(x = trending_date, ymin = bottom, ymax = top), alpha = 0.6)+
  ylab("Views") +
  xlab("Date")
```

In this plot, each vertical line indicates 93.3% confidence interval bands. It is interesting to see that although YouTube videos with the patterns have higher average views, the variance for these videos are larger than the videos without the patterns. This result in overlaping confidence intervals for most of the time, which lead to lack of significance to conclude that videos with pattern will have higher views on average. However, for some period from late April to early June, the difference bettwen views of videos with pattern in title appears to be significant different from the ones without the pattern. 

# Section: Time takes for a video to become popular

```{r warning=FALSE}
### US data
Summary_Stats <-
  US_data %>%
  select(video_id, publish_time, trending_date, category_name) %>%
  mutate(publish_time = as.Date(publish_time)) %>%
  group_by(video_id) %>%
  mutate(first_trending_date = min(trending_date), days_to_trending = interval(publish_time, first_trending_date)/86400)
```


```{r}
Summary_Stats %>%
  group_by(category_name) %>%
  mutate(mean_days = mean(days_to_trending), se = sd(days_to_trending)/sqrt(n())) %>%
  mutate(top = mean_days + 2*se, bottom = mean_days - 2 * se) %>%
  ggplot(aes(x = category_name, y = days_to_trending)) +
  geom_bar(data = Summary_Stats, stat = "identity", alpha = 0.2 ) +
  geom_errorbar(aes(x = category_name, ymax = top, ymin = bottom))#+
#  ylim(c(0,90))

```

```{r}
KR_data %>%
 select(category_name, trending_date, video_id) %>%
 filter(trending_date == ymd(20180328))

```